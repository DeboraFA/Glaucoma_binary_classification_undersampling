{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ec272",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10968,
     "status": "ok",
     "timestamp": 1655562800010,
     "user": {
      "displayName": "Débora Ferreira",
      "userId": "05060274424742616333"
     },
     "user_tz": 180
    },
    "id": "f95ec272",
    "outputId": "2e3da4fd-4ff5-40b7-85d5-6aefbc6c90af"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy --upgrade\n",
    "# !pip install mahotas\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import mahotas\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from skimage.exposure import is_low_contrast\n",
    "from skimage.filters import gabor_kernel\n",
    "from scipy.stats import kurtosis, skew\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import exposure, feature, color\n",
    "from skimage.feature import hog\n",
    "from scipy import ndimage as ndi\n",
    "from pywt import dwt2\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "from crossdata_metrics import metrics, data_balanced, train_crossdataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f14de",
   "metadata": {
    "id": "571f14de"
   },
   "source": [
    "# Features extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f094903a",
   "metadata": {
    "id": "f094903a"
   },
   "outputs": [],
   "source": [
    "kernels = []\n",
    "for theta in range(2):\n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (2,3):\n",
    "        for frequency in (0.05, 0.25):\n",
    "            kernel = np.real(gabor_kernel(frequency, theta=theta,\n",
    "                                              sigma_x=sigma, sigma_y=sigma))\n",
    "            kernels.append(kernel)\n",
    "\n",
    "def gabor_feat(image):\n",
    "\n",
    "    feats = np.zeros((len(kernels), 3), dtype=np.double)\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        filtered = ndi.convolve(image, kernel, mode='wrap')\n",
    "        feats[k, 0] = filtered.mean() # Média\n",
    "        feats[k, 1] = skew(filtered.flatten()) # Assimetria\n",
    "        feats[k, 2] = kurtosis(filtered.flatten()) # Curtose\n",
    "        _, (cH, cV, cD) = dwt2(filtered.T, 'db1')\n",
    "        r_feat = np.hstack([feats[0],feats[1],feats[2],feats[3]])\n",
    "\n",
    "    return r_feat \n",
    "\n",
    "\n",
    "\n",
    "# Momentos Zernike \n",
    "def ZernikeMoments(image):\n",
    "    gray = image\n",
    "    rows, cols = gray.shape\n",
    "    radius = cols//2 if rows > cols else rows//2\n",
    "    zernike = mahotas.features.zernike_moments(gray,radius)\n",
    "    return zernike    \n",
    "\n",
    "\n",
    "# Histogram of Oriented Gradients (HOG)\n",
    "def fd_hog_gradients(image):\n",
    "\n",
    "    fd, hog_image = feature.hog(image, orientations=8, pixels_per_cell=(128, 128),cells_per_block=(2, 2),\n",
    "                                transform_sqrt=True , block_norm=\"L1\",visualize=True)\n",
    "    return fd\n",
    "\n",
    "\n",
    "# LBP - Local Binary Pattern \n",
    "radius = 8\n",
    "numPoints = 64\n",
    "def describeLBP(image, eps=1e-7):\n",
    "    lbp = feature.local_binary_pattern(image,numPoints,radius, method=\"uniform\")\n",
    "    (hist, _) = np.histogram(lbp.ravel(),bins=np.arange(0, numPoints + 3),range=(0, numPoints + 2))\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250dd89e",
   "metadata": {
    "id": "250dd89e"
   },
   "source": [
    "# Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd36f54b",
   "metadata": {
    "id": "fd36f54b"
   },
   "outputs": [],
   "source": [
    "def load_path(path):\n",
    "    labels_ = os.listdir(path)\n",
    "    features = []\n",
    "    labels   = []\n",
    "    for i, label in enumerate(labels_):\n",
    "        cur_path = path + \"/\" + label \n",
    "        for image_path in glob.glob(cur_path + \"/*\"):\n",
    "            img = cv2.imread(image_path)\n",
    "            image = cv2.resize(img, (256,256))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            image2 = image/255\n",
    "            lbp = describeLBP(image2) # LBP\n",
    "            hog = fd_hog_gradients(image2) # HOG \n",
    "            zernike = ZernikeMoments(image2) # Momentos de Zernike\n",
    "            gabor= gabor_feat(image2) # Filtros de Gabor \n",
    "\n",
    "            global_feature = np.hstack([hog,lbp, zernike, gabor])\n",
    "            features.append(global_feature)\n",
    "            labels.append(label)            \n",
    "        print (\"[INFO] completed label - \" + label)\n",
    "    return features, labels\n",
    "### ACRIMA\n",
    "ACRIMA_data, ACRIMA_labels = load_path(\"D:/Glaucoma/Banco_de_dados/ACRIMA/Images\")\n",
    "\n",
    "## REFUGE\n",
    "REFUGE_data_train, REFUGE_labels_train = load_path(\"D:/Glaucoma/Banco_de_dados/REFUGE/cortes2\")\n",
    "REFUGE_data_test, REFUGE_labels_test = load_path(\"D:/Glaucoma/Banco_de_dados/REFUGE/REFUGE-Test400/Cortes_test\")\n",
    "\n",
    "#### RIM-ONE \n",
    "RO_data_train, RO_labels_train = load_path(\"D:/Glaucoma/Banco_de_dados/RIM-ONE_DL_images/partitioned_randomly/training_set\")\n",
    "RO_data_test, RO_labels_test = load_path(\"D:/Glaucoma/Banco_de_dados/RIM-ONE_DL_images/partitioned_randomly/test_set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Refuge + Acrima + Rim-One DL\n",
    "data = np.concatenate([ACRIMA_data, REFUGE_data_train, REFUGE_data_test, RO_data_train,RO_data_test], axis = 0)\n",
    "labels_ = np.concatenate([ACRIMA_labels, REFUGE_labels_train, REFUGE_labels_test, RO_labels_train, RO_labels_test], axis = 0)\n",
    "\n",
    "# Refuge + Acrima\n",
    "data_re_ac = np.concatenate([ACRIMA_data, REFUGE_data_train, REFUGE_data_test], axis = 0)\n",
    "labels_re_ac_ = np.concatenate([ACRIMA_labels, REFUGE_labels_train, REFUGE_labels_test], axis = 0)\n",
    "\n",
    "# Rim-One + Acrima\n",
    "data_ro_ac = np.concatenate([ACRIMA_data, RO_data_train,RO_data_test], axis = 0)\n",
    "labels_ro_ac_ = np.concatenate([ACRIMA_labels, RO_labels_train, RO_labels_test], axis = 0)\n",
    "\n",
    "# Rim-One + Refuge\n",
    "data_ro_re = np.concatenate([REFUGE_data_train, REFUGE_data_test, RO_data_train,RO_data_test], axis = 0)\n",
    "labels_ro_re_ = np.concatenate([REFUGE_labels_train, REFUGE_labels_test, RO_labels_train, RO_labels_test], axis = 0)\n",
    "\n",
    "# Refuge\n",
    "data_re = np.concatenate([REFUGE_data_train, REFUGE_data_test], axis = 0)\n",
    "labels_re_ = np.concatenate([REFUGE_labels_train, REFUGE_labels_test], axis = 0)\n",
    "\n",
    "# Acrima\n",
    "data_ac = np.concatenate([ACRIMA_data], axis = 0)\n",
    "labels_ac_ = np.concatenate([ACRIMA_labels], axis = 0)\n",
    "\n",
    "# Rim-One\n",
    "data_ro = np.concatenate([RO_data_train,RO_data_test], axis = 0)\n",
    "labels_ro_ = np.concatenate([RO_labels_train, RO_labels_test], axis = 0)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels_)\n",
    "labels_re_ac = le.fit_transform(labels_re_ac_)\n",
    "labels_ro_ac = le.fit_transform(labels_ro_ac_)\n",
    "labels_ro_re = le.fit_transform(labels_ro_re_)\n",
    "labels_re = le.fit_transform(labels_re_)\n",
    "labels_ac = le.fit_transform(labels_ac_)\n",
    "labels_ro = le.fit_transform(labels_ro_)\n",
    "\n",
    "print(\"Data: \", np.shape(data))\n",
    "print(\"Normal: \", len(data[labels==0]))\n",
    "print(\"Glaucoma: \", len(data[labels==1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f529ea5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e1aacda",
   "metadata": {},
   "source": [
    "# Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99150e91",
   "metadata": {
    "id": "99150e91"
   },
   "outputs": [],
   "source": [
    "svm = SVC(C=3, kernel='rbf', probability=True)\n",
    "mlp = MLPClassifier(activation='relu',max_iter=500,alpha=0.001,hidden_layer_sizes=(40,),learning_rate='constant')\n",
    "xgb = XGBClassifier(colsample_bytree= 1, max_depth=5, n_estimators=300, subsample= 0.6, seed=0)\n",
    "\n",
    "models = []\n",
    "# models.append(('SVM', SVC(C=3, kernel='rbf')))\n",
    "# models.append(('XGB', XGBClassifier(colsample_bytree= 1, max_depth=5, n_estimators= 100, subsample= 0.6, seed=0))) \n",
    "# models.append(('MLP', MLPClassifier(activation='relu',max_iter=1000,alpha=0.01,hidden_layer_sizes=(40,40,40),learning_rate='constant')))\n",
    "models.append(('VOT', VotingClassifier(estimators = [('svm', svm),('mlp', mlp),('xgb', xgb)], voting='hard')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af8a195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb3136b2",
   "metadata": {
    "id": "cb3136b2"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baebc8e3",
   "metadata": {
    "id": "839341f8"
   },
   "source": [
    "## Unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec16922",
   "metadata": {
    "id": "3ec16922",
    "outputId": "0ca2b64e-b9eb-479f-d38b-837d6a392aec"
   },
   "outputs": [],
   "source": [
    "def results_unbalanced():\n",
    "    train_crossdataset(data_ro_ac, labels_ro_ac, data_re, labels_re, 'None', models, 'scaler')\n",
    "    train_crossdataset(data_ro_re, labels_ro_re, data_ac, labels_ac, 'None', models, 'scaler')\n",
    "    train_crossdataset(data_re_ac, labels_re_ac, data_ro, labels_ro, 'None', models, 'scaler')\n",
    "    return \n",
    "\n",
    "results_unbalanced()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c446c379",
   "metadata": {},
   "source": [
    "## Random Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1245d981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanceamento aleatório\n",
    "def results_balanced_random():\n",
    "\n",
    "    train_crossdataset(data_ro_ac, labels_ro_ac, data_re, labels_re, 'random', models, 'scaler')\n",
    "    train_crossdataset(data_ro_re, labels_ro_re, data_ac, labels_ac, 'random', models, 'scaler')\n",
    "    train_crossdataset(data_re_ac, labels_re_ac, data_ro, labels_ro, 'random', models, 'scaler')\n",
    "    \n",
    "    return \n",
    "    \n",
    "results_balanced_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482b854",
   "metadata": {},
   "source": [
    "## Near Miss Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cekYwcpVE3rp",
   "metadata": {
    "id": "cekYwcpVE3rp"
   },
   "outputs": [],
   "source": [
    "# Balanceamento Near Miss\n",
    "def results_balanced_nm():\n",
    "    train_crossdataset(data_ro_ac, labels_ro_ac, data_re, labels_re, 'nm', models, 'scaler')\n",
    "    train_crossdataset(data_ro_re, labels_ro_re, data_ac, labels_ac, 'nm', models, 'scaler')\n",
    "    train_crossdataset(data_re_ac, labels_re_ac, data_ro, labels_ro, 'nm', models, 'scaler')\n",
    "    return \n",
    "    \n",
    "results_balanced_nm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd7c7a",
   "metadata": {},
   "source": [
    "## Cluster Centroid Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf1325",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Balanceamento cluster\n",
    "\n",
    "def results_balanced_cluster():\n",
    "    train_crossdataset(data_ro_ac, labels_ro_ac, data_re, labels_re, 'cluster', models, 'scaler')\n",
    "    train_crossdataset(data_ro_re, labels_ro_re, data_ac, labels_ac, 'cluster', models, 'scaler')\n",
    "    train_crossdataset(data_re_ac, labels_re_ac, data_ro, labels_ro, 'cluster', models, 'scaler')\n",
    "    \n",
    "    return \n",
    "    \n",
    "results_balanced_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ed750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classificacao_modelo_tradicional.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
